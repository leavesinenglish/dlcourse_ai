{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNxWNutq1zXM"
      },
      "source": [
        "# Задание 6: Рекуррентные нейронные сети (RNNs)\n",
        "\n",
        "Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P59NYU98GCb9",
        "outputId": "7c136612-cbc5-4ba0-abe8-c2002dbc6aee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==0.4.1 (from versions: 1.0.0, 1.0.1, 1.0.1.post2, 1.1.0, 1.2.0, 1.3.0, 1.3.1, 1.4.0, 1.5.0, 1.5.1, 1.6.0, 1.7.0, 1.7.1, 1.8.0, 1.8.1, 1.9.0, 1.9.1, 1.10.0, 1.10.1, 1.10.2, 1.11.0)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for torch==0.4.1\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 16.0 MB 25.1 MB/s \n",
            "\u001b[?25h  Building wheel for bokeh (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "panel 0.12.1 requires bokeh<2.4.0,>=2.3.0, but you have bokeh 0.13.0 which is incompatible.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 5.4 MB 34.0 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.20.2 which is incompatible.\n",
            "imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.20.2 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip3 -qq install torch==0.4.1\n",
        "!pip3 -qq install bokeh==0.13.0\n",
        "!pip3 -qq install gensim==3.6.0\n",
        "!pip3 -qq install nltk\n",
        "!pip3 -qq install scikit-learn==0.20.2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "8sVtGHmA9aBM"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    from torch.cuda import FloatTensor, LongTensor\n",
        "else:\n",
        "    from torch import FloatTensor, LongTensor\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-6CNKM3b4hT1"
      },
      "source": [
        "# Рекуррентные нейронные сети (RNNs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_XkoGNQUeGm"
      },
      "source": [
        "## POS Tagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFEtWrS_4rUs"
      },
      "source": [
        "Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n",
        "\n",
        "![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
        "\n",
        "*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n",
        "\n",
        "Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n",
        "\n",
        "Мы порешаем сейчас POS Tagging для английского.\n",
        "\n",
        "Будем работать с таким набором тегов:\n",
        "- ADJ - adjective (new, good, high, ...)\n",
        "- ADP - adposition (on, of, at, ...)\n",
        "- ADV - adverb (really, already, still, ...)\n",
        "- CONJ - conjunction (and, or, but, ...)\n",
        "- DET - determiner, article (the, a, some, ...)\n",
        "- NOUN - noun (year, home, costs, ...)\n",
        "- NUM - numeral (twenty-four, fourth, 1991, ...)\n",
        "- PRT - particle (at, on, out, ...)\n",
        "- PRON - pronoun (he, their, her, ...)\n",
        "- VERB - verb (is, say, told, ...)\n",
        "- . - punctuation marks (. , ;)\n",
        "- X - other (ersatz, esprit, dunno, ...)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPIkKdFlHB-X"
      },
      "source": [
        "Скачаем данные:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiA2dGmgF1rW",
        "outputId": "01a4feba-40ec-45be-dfbb-052498d94353"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/image.py:167: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dtype=np.int):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:35: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:597: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:836: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, fit_path=True,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:862: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1097: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1344: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  max_n_alphas=1000, n_jobs=None, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/least_angle.py:1480: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, copy_X=True, positive=False):\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:152: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  precompute=False, eps=np.finfo(np.float).eps,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:320: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=np.finfo(np.float).eps, random_state=None,\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/randomized_l1.py:580: DeprecationWarning: `np.float` is a deprecated alias for the builtin `float`. To silence this warning, use `float` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.float64` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  eps=4 * np.finfo(np.float).eps, n_jobs=None,\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "\n",
        "data = nltk.corpus.brown.tagged_sents(tagset='universal')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d93g_swyJA_V"
      },
      "source": [
        "Пример размеченного предложения:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QstS4NO0L97c",
        "outputId": "3e83aaf9-3c3b-4889-c6d6-f50dddae5340"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The            \tDET\n",
            "Fulton         \tNOUN\n",
            "County         \tNOUN\n",
            "Grand          \tADJ\n",
            "Jury           \tNOUN\n",
            "said           \tVERB\n",
            "Friday         \tNOUN\n",
            "an             \tDET\n",
            "investigation  \tNOUN\n",
            "of             \tADP\n",
            "Atlanta's      \tNOUN\n",
            "recent         \tADJ\n",
            "primary        \tNOUN\n",
            "election       \tNOUN\n",
            "produced       \tVERB\n",
            "``             \t.\n",
            "no             \tDET\n",
            "evidence       \tNOUN\n",
            "''             \t.\n",
            "that           \tADP\n",
            "any            \tDET\n",
            "irregularities \tNOUN\n",
            "took           \tVERB\n",
            "place          \tNOUN\n",
            ".              \t.\n"
          ]
        }
      ],
      "source": [
        "for word, tag in data[0]:\n",
        "    print('{:15}\\t{}'.format(word, tag))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epdW8u_YXcAv"
      },
      "source": [
        "Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n",
        "\n",
        "На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTai8Ta0lgwL",
        "outputId": "0c53be23-02bb-484e-bffa-9c54f0f313a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Words count in train set: 739769\n",
            "Words count in val set: 130954\n",
            "Words count in test set: 290469\n"
          ]
        }
      ],
      "source": [
        "train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n",
        "train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n",
        "\n",
        "print('Words count in train set:', sum(len(sent) for sent in train_data))\n",
        "print('Words count in val set:', sum(len(sent) for sent in val_data))\n",
        "print('Words count in test set:', sum(len(sent) for sent in test_data))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eChdLNGtXyP0"
      },
      "source": [
        "Построим маппинги из слов в индекс и из тега в индекс:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjwwDs6Zq9x",
        "outputId": "364a3985-b7be-4116-b40a-b92ba5a943d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique words in train = 45441. Tags = {'PRON', 'DET', '.', 'ADP', 'ADV', 'CONJ', 'ADJ', 'NUM', 'VERB', 'NOUN', 'PRT', 'X'}\n"
          ]
        }
      ],
      "source": [
        "words = {word for sample in train_data for word, tag in sample}\n",
        "word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n",
        "word2ind['<pad>'] = 0\n",
        "\n",
        "tags = {tag for sample in train_data for word, tag in sample}\n",
        "tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n",
        "tag2ind['<pad>'] = 0\n",
        "\n",
        "print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "URC1B2nvPGFt",
        "outputId": "7dc6bc75-fe4e-4c56-eada-433bd0ee0813"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x360 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n",
        "tag_distribution = [tag_distribution[tag] for tag in tags]\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "bar_width = 0.35\n",
        "plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n",
        "plt.xticks(np.arange(len(tags)), tags)\n",
        "    \n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gArQwbzWWkgi"
      },
      "source": [
        "## Бейзлайн\n",
        "\n",
        "Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n",
        "\n",
        "![tag-context](https://www.nltk.org/images/tag-context.png)  \n",
        "*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n",
        "\n",
        "На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n",
        "\n",
        "Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n",
        "\n",
        "Простейший вариант - униграммная модель, учитывающая только слово:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rWmSToIaeAo",
        "outputId": "5a4dc898-e058-43b1-f3b6-f02099ee15a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of unigram tagger = 92.62%\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "\n",
        "default_tagger = nltk.DefaultTagger('NN')\n",
        "\n",
        "unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n",
        "print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07Ymb_MkbWsF"
      },
      "source": [
        "Добавим вероятности переходов:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vjz_Rk0bbMyH",
        "outputId": "dcab0e20-08f6-4763-8a15-e8fe5bf2ea46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of bigram tagger = 93.42%\n"
          ]
        }
      ],
      "source": [
        "bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n",
        "print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWMw6QHvbaDd"
      },
      "source": [
        "Обратите внимание, что `backoff` важен:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8XCuxEBVbOY_",
        "outputId": "23fe0633-192c-40a9-86f8-6058113ea375"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of trigram tagger = 23.33%\n"
          ]
        }
      ],
      "source": [
        "trigram_tagger = nltk.TrigramTagger(train_data)\n",
        "print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4t3xyYd__8d-"
      },
      "source": [
        "## Увеличиваем контекст с рекуррентными сетями\n",
        "\n",
        "Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n",
        "\n",
        "Омонимия - основная причина, почему униграмная модель плоха:  \n",
        "*“he cashed a check at the **bank**”*  \n",
        "vs  \n",
        "*“he sat on the **bank** of the river”*\n",
        "\n",
        "Поэтому нам очень полезно учитывать контекст при предсказании тега.\n",
        "\n",
        "Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n",
        "\n",
        "![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n",
        "\n",
        "Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "RtRbz1SwgEqc"
      },
      "outputs": [],
      "source": [
        "def convert_data(data, word2ind, tag2ind):\n",
        "    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n",
        "    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n",
        "    \n",
        "    return X, y\n",
        "\n",
        "X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n",
        "X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n",
        "X_test, y_test = convert_data(test_data, word2ind, tag2ind)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "DhsTKZalfih6"
      },
      "outputs": [],
      "source": [
        "def iterate_batches(data, batch_size):\n",
        "    X, y = data\n",
        "    n_samples = len(X)\n",
        "\n",
        "    indices = np.arange(n_samples)\n",
        "    np.random.shuffle(indices)\n",
        "    \n",
        "    for start in range(0, n_samples, batch_size):\n",
        "        end = min(start + batch_size, n_samples)\n",
        "        \n",
        "        batch_indices = indices[start:end]\n",
        "        \n",
        "        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n",
        "        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n",
        "        \n",
        "        for batch_ind, sample_ind in enumerate(batch_indices):\n",
        "            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n",
        "            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n",
        "            \n",
        "        yield X_batch, y_batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4XsRII5kW5x",
        "outputId": "8d99a092-2248-4d2f-a492-9af5873500e9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((32, 4), (32, 4))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n",
        "\n",
        "X_batch.shape, y_batch.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5I9E9P6eFYv"
      },
      "source": [
        "**Задание** Реализуйте `LSTMTagger`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "WVEHju54d68T"
      },
      "outputs": [],
      "source": [
        "class LSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.FC = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        out, _ = self.lstm(embedding)\n",
        "        return self.FC(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_HA8zyheYGH"
      },
      "source": [
        "**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbrxsZ2mehWB",
        "outputId": "16bcd6db-9b2b-4bcd-b068-90e311cf0329"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8, 92)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ")\n",
        "\n",
        "X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n",
        "\n",
        "logits = model(X_batch)\n",
        "\n",
        "def accuracy(pred, gt):\n",
        "    _, indices = torch.max(pred, -1)\n",
        "    total = torch.sum(gt>0).item()\n",
        "    corr = torch.sum((indices == gt)*(gt>0)).item()\n",
        "    return corr, total\n",
        "\n",
        "accuracy(logits, y_batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMUyUm1hgpe3",
        "outputId": "5c9a8d91-dc04-4af8-d042-f52aafd1069a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.5623, grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "criterion(logits.reshape((-1,len(tag2ind))), y_batch.view(-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSgV3NPUpcjH"
      },
      "source": [
        "**Задание** Вставьте эти вычисление в функцию:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FprPQ0gllo7b"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n",
        "    epoch_loss = 0\n",
        "    correct_count = 0\n",
        "    sum_count = 0\n",
        "    \n",
        "    is_train = not optimizer is None\n",
        "    name = name or ''\n",
        "    model.train(is_train)\n",
        "    \n",
        "    batches_count = math.ceil(len(data[0]) / batch_size)\n",
        "    \n",
        "    with torch.autograd.set_grad_enabled(is_train):\n",
        "        with tqdm(total=batches_count) as progress_bar:\n",
        "            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n",
        "                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "                logits = model(X_batch)\n",
        "\n",
        "                loss = criterion(logits.reshape((-1, len(tag2ind))), y_batch.view(-1))\n",
        "\n",
        "                epoch_loss += loss.item()\n",
        "\n",
        "                if optimizer:\n",
        "                    optimizer.zero_grad()\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "\n",
        "                cur_correct_count, cur_sum_count = accuracy(logits, y_batch)\n",
        "\n",
        "                correct_count += cur_correct_count\n",
        "                sum_count += cur_sum_count\n",
        "\n",
        "                progress_bar.update()\n",
        "                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                    name, loss.item(), cur_correct_count / cur_sum_count)\n",
        "                )\n",
        "                \n",
        "            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n",
        "                name, epoch_loss / batches_count, correct_count / sum_count)\n",
        "            )\n",
        "\n",
        "    return epoch_loss / batches_count, correct_count / sum_count\n",
        "\n",
        "\n",
        "def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n",
        "        val_data=None, val_batch_size=None):\n",
        "        \n",
        "    if not val_data is None and val_batch_size is None:\n",
        "        val_batch_size = batch_size\n",
        "        \n",
        "    for epoch in range(epochs_count):\n",
        "        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n",
        "        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n",
        "        \n",
        "        if not val_data is None:\n",
        "            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pqfbeh1ltEYa",
        "outputId": "5e060c0d-9c1f-4e05-d191-24464744923c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.31205, Accuracy = 71.88%: 100%|██████████| 572/572 [00:05<00:00, 95.65it/s] \n",
            "[1 / 50]   Val: Loss = 0.10814, Accuracy = 84.71%: 100%|██████████| 13/13 [00:00<00:00, 77.74it/s]\n",
            "[2 / 50] Train: Loss = 0.10100, Accuracy = 89.96%: 100%|██████████| 572/572 [00:05<00:00, 100.87it/s]\n",
            "[2 / 50]   Val: Loss = 0.07609, Accuracy = 89.48%: 100%|██████████| 13/13 [00:00<00:00, 82.66it/s]\n",
            "[3 / 50] Train: Loss = 0.06744, Accuracy = 93.25%: 100%|██████████| 572/572 [00:05<00:00, 101.55it/s]\n",
            "[3 / 50]   Val: Loss = 0.06847, Accuracy = 91.21%: 100%|██████████| 13/13 [00:00<00:00, 77.97it/s]\n",
            "[4 / 50] Train: Loss = 0.05053, Accuracy = 94.86%: 100%|██████████| 572/572 [00:05<00:00, 98.06it/s]\n",
            "[4 / 50]   Val: Loss = 0.06700, Accuracy = 92.11%: 100%|██████████| 13/13 [00:00<00:00, 76.23it/s]\n",
            "[5 / 50] Train: Loss = 0.04048, Accuracy = 95.85%: 100%|██████████| 572/572 [00:06<00:00, 87.31it/s]\n",
            "[5 / 50]   Val: Loss = 0.06537, Accuracy = 92.67%: 100%|██████████| 13/13 [00:00<00:00, 79.95it/s]\n",
            "[6 / 50] Train: Loss = 0.03285, Accuracy = 96.60%: 100%|██████████| 572/572 [00:05<00:00, 100.59it/s]\n",
            "[6 / 50]   Val: Loss = 0.06315, Accuracy = 92.97%: 100%|██████████| 13/13 [00:00<00:00, 82.19it/s]\n",
            "[7 / 50] Train: Loss = 0.02721, Accuracy = 97.19%: 100%|██████████| 572/572 [00:05<00:00, 100.43it/s]\n",
            "[7 / 50]   Val: Loss = 0.06457, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 78.58it/s]\n",
            "[8 / 50] Train: Loss = 0.02254, Accuracy = 97.67%: 100%|██████████| 572/572 [00:05<00:00, 98.85it/s] \n",
            "[8 / 50]   Val: Loss = 0.06511, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 80.04it/s]\n",
            "[9 / 50] Train: Loss = 0.01864, Accuracy = 98.05%: 100%|██████████| 572/572 [00:05<00:00, 98.69it/s]\n",
            "[9 / 50]   Val: Loss = 0.07238, Accuracy = 93.38%: 100%|██████████| 13/13 [00:00<00:00, 82.78it/s]\n",
            "[10 / 50] Train: Loss = 0.01561, Accuracy = 98.38%: 100%|██████████| 572/572 [00:05<00:00, 100.79it/s]\n",
            "[10 / 50]   Val: Loss = 0.07432, Accuracy = 93.38%: 100%|██████████| 13/13 [00:00<00:00, 83.65it/s]\n",
            "[11 / 50] Train: Loss = 0.01286, Accuracy = 98.68%: 100%|██████████| 572/572 [00:05<00:00, 99.12it/s]\n",
            "[11 / 50]   Val: Loss = 0.06838, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 77.69it/s]\n",
            "[12 / 50] Train: Loss = 0.01069, Accuracy = 98.92%: 100%|██████████| 572/572 [00:05<00:00, 98.61it/s]\n",
            "[12 / 50]   Val: Loss = 0.07646, Accuracy = 93.43%: 100%|██████████| 13/13 [00:00<00:00, 80.81it/s]\n",
            "[13 / 50] Train: Loss = 0.00864, Accuracy = 99.15%: 100%|██████████| 572/572 [00:05<00:00, 98.27it/s] \n",
            "[13 / 50]   Val: Loss = 0.07479, Accuracy = 93.32%: 100%|██████████| 13/13 [00:00<00:00, 77.72it/s]\n",
            "[14 / 50] Train: Loss = 0.00718, Accuracy = 99.29%: 100%|██████████| 572/572 [00:05<00:00, 97.21it/s] \n",
            "[14 / 50]   Val: Loss = 0.07696, Accuracy = 93.30%: 100%|██████████| 13/13 [00:00<00:00, 75.07it/s]\n",
            "[15 / 50] Train: Loss = 0.00579, Accuracy = 99.45%: 100%|██████████| 572/572 [00:05<00:00, 98.55it/s]\n",
            "[15 / 50]   Val: Loss = 0.08642, Accuracy = 93.32%: 100%|██████████| 13/13 [00:00<00:00, 77.79it/s]\n",
            "[16 / 50] Train: Loss = 0.00471, Accuracy = 99.56%: 100%|██████████| 572/572 [00:05<00:00, 98.46it/s]\n",
            "[16 / 50]   Val: Loss = 0.08634, Accuracy = 93.26%: 100%|██████████| 13/13 [00:00<00:00, 77.17it/s]\n",
            "[17 / 50] Train: Loss = 0.00396, Accuracy = 99.64%: 100%|██████████| 572/572 [00:05<00:00, 98.72it/s]\n",
            "[17 / 50]   Val: Loss = 0.09009, Accuracy = 93.29%: 100%|██████████| 13/13 [00:00<00:00, 79.08it/s]\n",
            "[18 / 50] Train: Loss = 0.00327, Accuracy = 99.71%: 100%|██████████| 572/572 [00:05<00:00, 97.73it/s]\n",
            "[18 / 50]   Val: Loss = 0.10231, Accuracy = 93.22%: 100%|██████████| 13/13 [00:00<00:00, 83.44it/s]\n",
            "[19 / 50] Train: Loss = 0.00287, Accuracy = 99.74%: 100%|██████████| 572/572 [00:05<00:00, 97.86it/s] \n",
            "[19 / 50]   Val: Loss = 0.09965, Accuracy = 93.24%: 100%|██████████| 13/13 [00:00<00:00, 80.20it/s]\n",
            "[20 / 50] Train: Loss = 0.00252, Accuracy = 99.77%: 100%|██████████| 572/572 [00:05<00:00, 98.34it/s]\n",
            "[20 / 50]   Val: Loss = 0.10645, Accuracy = 93.13%: 100%|██████████| 13/13 [00:00<00:00, 80.75it/s]\n",
            "[21 / 50] Train: Loss = 0.00215, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 99.26it/s] \n",
            "[21 / 50]   Val: Loss = 0.10472, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 75.44it/s]\n",
            "[22 / 50] Train: Loss = 0.00205, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 95.89it/s]\n",
            "[22 / 50]   Val: Loss = 0.11545, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 80.91it/s]\n",
            "[23 / 50] Train: Loss = 0.00202, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 97.52it/s] \n",
            "[23 / 50]   Val: Loss = 0.11377, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 77.18it/s]\n",
            "[24 / 50] Train: Loss = 0.00201, Accuracy = 99.80%: 100%|██████████| 572/572 [00:05<00:00, 95.71it/s]\n",
            "[24 / 50]   Val: Loss = 0.12834, Accuracy = 93.12%: 100%|██████████| 13/13 [00:00<00:00, 83.55it/s]\n",
            "[25 / 50] Train: Loss = 0.00184, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 95.35it/s]\n",
            "[25 / 50]   Val: Loss = 0.11408, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 77.98it/s]\n",
            "[26 / 50] Train: Loss = 0.00177, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 96.50it/s]\n",
            "[26 / 50]   Val: Loss = 0.11889, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 78.29it/s]\n",
            "[27 / 50] Train: Loss = 0.00176, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 94.88it/s]\n",
            "[27 / 50]   Val: Loss = 0.12558, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 80.25it/s]\n",
            "[28 / 50] Train: Loss = 0.00177, Accuracy = 99.81%: 100%|██████████| 572/572 [00:05<00:00, 96.27it/s]\n",
            "[28 / 50]   Val: Loss = 0.12285, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 70.44it/s]\n",
            "[29 / 50] Train: Loss = 0.00156, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 95.12it/s]\n",
            "[29 / 50]   Val: Loss = 0.13074, Accuracy = 93.19%: 100%|██████████| 13/13 [00:00<00:00, 75.38it/s]\n",
            "[30 / 50] Train: Loss = 0.00152, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.70it/s]\n",
            "[30 / 50]   Val: Loss = 0.13208, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 74.67it/s]\n",
            "[31 / 50] Train: Loss = 0.00145, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 97.23it/s]\n",
            "[31 / 50]   Val: Loss = 0.13214, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 77.08it/s]\n",
            "[32 / 50] Train: Loss = 0.00159, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 95.95it/s]\n",
            "[32 / 50]   Val: Loss = 0.13348, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 74.96it/s]\n",
            "[33 / 50] Train: Loss = 0.00225, Accuracy = 99.75%: 100%|██████████| 572/572 [00:05<00:00, 96.60it/s]\n",
            "[33 / 50]   Val: Loss = 0.13835, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 78.26it/s]\n",
            "[34 / 50] Train: Loss = 0.00155, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 95.02it/s]\n",
            "[34 / 50]   Val: Loss = 0.14621, Accuracy = 93.10%: 100%|██████████| 13/13 [00:00<00:00, 79.27it/s]\n",
            "[35 / 50] Train: Loss = 0.00139, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 94.17it/s]\n",
            "[35 / 50]   Val: Loss = 0.13162, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 73.23it/s]\n",
            "[36 / 50] Train: Loss = 0.00130, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.58it/s]\n",
            "[36 / 50]   Val: Loss = 0.15324, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 79.54it/s]\n",
            "[37 / 50] Train: Loss = 0.00135, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.79it/s]\n",
            "[37 / 50]   Val: Loss = 0.14981, Accuracy = 93.16%: 100%|██████████| 13/13 [00:00<00:00, 79.55it/s]\n",
            "[38 / 50] Train: Loss = 0.00149, Accuracy = 99.83%: 100%|██████████| 572/572 [00:05<00:00, 95.36it/s]\n",
            "[38 / 50]   Val: Loss = 0.13863, Accuracy = 93.03%: 100%|██████████| 13/13 [00:00<00:00, 75.22it/s]\n",
            "[39 / 50] Train: Loss = 0.00213, Accuracy = 99.76%: 100%|██████████| 572/572 [00:06<00:00, 93.44it/s]\n",
            "[39 / 50]   Val: Loss = 0.16413, Accuracy = 93.08%: 100%|██████████| 13/13 [00:00<00:00, 78.12it/s]\n",
            "[40 / 50] Train: Loss = 0.00142, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 95.23it/s]\n",
            "[40 / 50]   Val: Loss = 0.15366, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 77.67it/s]\n",
            "[41 / 50] Train: Loss = 0.00133, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 95.00it/s]\n",
            "[41 / 50]   Val: Loss = 0.15502, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 69.97it/s]\n",
            "[42 / 50] Train: Loss = 0.00133, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 95.24it/s]\n",
            "[42 / 50]   Val: Loss = 0.17179, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 78.87it/s]\n",
            "[43 / 50] Train: Loss = 0.00137, Accuracy = 99.84%: 100%|██████████| 572/572 [00:05<00:00, 95.56it/s]\n",
            "[43 / 50]   Val: Loss = 0.16068, Accuracy = 93.20%: 100%|██████████| 13/13 [00:00<00:00, 79.84it/s]\n",
            "[44 / 50] Train: Loss = 0.00144, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 94.18it/s]\n",
            "[44 / 50]   Val: Loss = 0.16096, Accuracy = 93.11%: 100%|██████████| 13/13 [00:00<00:00, 76.18it/s]\n",
            "[45 / 50] Train: Loss = 0.00155, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 94.19it/s]\n",
            "[45 / 50]   Val: Loss = 0.16193, Accuracy = 93.04%: 100%|██████████| 13/13 [00:00<00:00, 77.28it/s]\n",
            "[46 / 50] Train: Loss = 0.00150, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 94.86it/s]\n",
            "[46 / 50]   Val: Loss = 0.16392, Accuracy = 93.15%: 100%|██████████| 13/13 [00:00<00:00, 77.91it/s]\n",
            "[47 / 50] Train: Loss = 0.00134, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 93.18it/s]\n",
            "[47 / 50]   Val: Loss = 0.15866, Accuracy = 93.23%: 100%|██████████| 13/13 [00:00<00:00, 78.05it/s]\n",
            "[48 / 50] Train: Loss = 0.00129, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 94.14it/s]\n",
            "[48 / 50]   Val: Loss = 0.16526, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 76.02it/s]\n",
            "[49 / 50] Train: Loss = 0.00127, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 93.43it/s]\n",
            "[49 / 50]   Val: Loss = 0.16634, Accuracy = 93.18%: 100%|██████████| 13/13 [00:00<00:00, 78.41it/s]\n",
            "[50 / 50] Train: Loss = 0.00124, Accuracy = 99.84%: 100%|██████████| 572/572 [00:06<00:00, 93.94it/s]\n",
            "[50 / 50]   Val: Loss = 0.17283, Accuracy = 93.21%: 100%|██████████| 13/13 [00:00<00:00, 77.52it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m0qGetIhfUE5"
      },
      "source": [
        "### Masking\n",
        "\n",
        "**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n",
        "\n",
        "У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nAfV2dEOfHo5"
      },
      "source": [
        "**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "98wr38_rw55D",
        "outputId": "d4c05923-ac7e-4bf7-d228-390a40460e6a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9332045760477021"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXUTSFaEHbDG"
      },
      "source": [
        "### Bidirectional LSTM\n",
        "\n",
        "Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n",
        "\n",
        "![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n",
        "*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n",
        "\n",
        "**Задание** Добавьте Bidirectional LSTM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1GyrtB81zXk",
        "outputId": "28a2395d-2031-4418-be3a-8ca15248af6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.25538, Accuracy = 76.67%: 100%|██████████| 572/572 [00:06<00:00, 85.68it/s]\n",
            "[1 / 50]   Val: Loss = 0.07204, Accuracy = 89.32%: 100%|██████████| 13/13 [00:00<00:00, 59.14it/s]\n",
            "[2 / 50] Train: Loss = 0.07546, Accuracy = 92.59%: 100%|██████████| 572/572 [00:06<00:00, 86.41it/s]\n",
            "[2 / 50]   Val: Loss = 0.04766, Accuracy = 93.38%: 100%|██████████| 13/13 [00:00<00:00, 60.14it/s]\n",
            "[3 / 50] Train: Loss = 0.04903, Accuracy = 95.33%: 100%|██████████| 572/572 [00:06<00:00, 86.61it/s]\n",
            "[3 / 50]   Val: Loss = 0.04023, Accuracy = 94.79%: 100%|██████████| 13/13 [00:00<00:00, 61.59it/s]\n",
            "[4 / 50] Train: Loss = 0.03458, Accuracy = 96.75%: 100%|██████████| 572/572 [00:06<00:00, 85.77it/s]\n",
            "[4 / 50]   Val: Loss = 0.03700, Accuracy = 95.45%: 100%|██████████| 13/13 [00:00<00:00, 60.61it/s]\n",
            "[5 / 50] Train: Loss = 0.02546, Accuracy = 97.66%: 100%|██████████| 572/572 [00:06<00:00, 86.24it/s]\n",
            "[5 / 50]   Val: Loss = 0.03613, Accuracy = 95.81%: 100%|██████████| 13/13 [00:00<00:00, 62.85it/s]\n",
            "[6 / 50] Train: Loss = 0.01838, Accuracy = 98.31%: 100%|██████████| 572/572 [00:06<00:00, 86.32it/s]\n",
            "[6 / 50]   Val: Loss = 0.03572, Accuracy = 96.04%: 100%|██████████| 13/13 [00:00<00:00, 61.93it/s]\n",
            "[7 / 50] Train: Loss = 0.01317, Accuracy = 98.83%: 100%|██████████| 572/572 [00:06<00:00, 86.83it/s]\n",
            "[7 / 50]   Val: Loss = 0.03636, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 59.42it/s]\n",
            "[8 / 50] Train: Loss = 0.00920, Accuracy = 99.21%: 100%|██████████| 572/572 [00:06<00:00, 84.62it/s]\n",
            "[8 / 50]   Val: Loss = 0.03587, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 58.26it/s]\n",
            "[9 / 50] Train: Loss = 0.00616, Accuracy = 99.49%: 100%|██████████| 572/572 [00:06<00:00, 83.72it/s]\n",
            "[9 / 50]   Val: Loss = 0.03858, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 60.62it/s]\n",
            "[10 / 50] Train: Loss = 0.00412, Accuracy = 99.69%: 100%|██████████| 572/572 [00:06<00:00, 84.57it/s]\n",
            "[10 / 50]   Val: Loss = 0.03945, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 60.00it/s]\n",
            "[11 / 50] Train: Loss = 0.00254, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 84.35it/s]\n",
            "[11 / 50]   Val: Loss = 0.04031, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 57.22it/s]\n",
            "[12 / 50] Train: Loss = 0.00156, Accuracy = 99.91%: 100%|██████████| 572/572 [00:06<00:00, 83.74it/s]\n",
            "[12 / 50]   Val: Loss = 0.04586, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 61.74it/s]\n",
            "[13 / 50] Train: Loss = 0.00096, Accuracy = 99.96%: 100%|██████████| 572/572 [00:06<00:00, 82.63it/s]\n",
            "[13 / 50]   Val: Loss = 0.04326, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 59.01it/s]\n",
            "[14 / 50] Train: Loss = 0.00061, Accuracy = 99.98%: 100%|██████████| 572/572 [00:06<00:00, 83.66it/s]\n",
            "[14 / 50]   Val: Loss = 0.04733, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 58.80it/s]\n",
            "[15 / 50] Train: Loss = 0.00035, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 84.49it/s]\n",
            "[15 / 50]   Val: Loss = 0.05076, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 58.95it/s]\n",
            "[16 / 50] Train: Loss = 0.00023, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 84.96it/s]\n",
            "[16 / 50]   Val: Loss = 0.04908, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 57.68it/s]\n",
            "[17 / 50] Train: Loss = 0.00015, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.95it/s]\n",
            "[17 / 50]   Val: Loss = 0.05349, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 58.34it/s]\n",
            "[18 / 50] Train: Loss = 0.00198, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 83.83it/s]\n",
            "[18 / 50]   Val: Loss = 0.04917, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 58.56it/s]\n",
            "[19 / 50] Train: Loss = 0.00097, Accuracy = 99.91%: 100%|██████████| 572/572 [00:06<00:00, 83.77it/s]\n",
            "[19 / 50]   Val: Loss = 0.04972, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 57.59it/s]\n",
            "[20 / 50] Train: Loss = 0.00021, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 83.85it/s]\n",
            "[20 / 50]   Val: Loss = 0.05647, Accuracy = 96.29%: 100%|██████████| 13/13 [00:00<00:00, 60.69it/s]\n",
            "[21 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.44it/s]\n",
            "[21 / 50]   Val: Loss = 0.05774, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 61.88it/s]\n",
            "[22 / 50] Train: Loss = 0.00006, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.43it/s]\n",
            "[22 / 50]   Val: Loss = 0.05745, Accuracy = 96.32%: 100%|██████████| 13/13 [00:00<00:00, 57.66it/s]\n",
            "[23 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.24it/s]\n",
            "[23 / 50]   Val: Loss = 0.06061, Accuracy = 96.33%: 100%|██████████| 13/13 [00:00<00:00, 59.09it/s]\n",
            "[24 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.32it/s]\n",
            "[24 / 50]   Val: Loss = 0.05814, Accuracy = 96.31%: 100%|██████████| 13/13 [00:00<00:00, 55.04it/s]\n",
            "[25 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.07it/s]\n",
            "[25 / 50]   Val: Loss = 0.05825, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 59.56it/s]\n",
            "[26 / 50] Train: Loss = 0.00011, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 81.90it/s]\n",
            "[26 / 50]   Val: Loss = 0.06745, Accuracy = 95.63%: 100%|██████████| 13/13 [00:00<00:00, 61.01it/s]\n",
            "[27 / 50] Train: Loss = 0.00240, Accuracy = 99.75%: 100%|██████████| 572/572 [00:06<00:00, 83.13it/s]\n",
            "[27 / 50]   Val: Loss = 0.06118, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 60.27it/s]\n",
            "[28 / 50] Train: Loss = 0.00036, Accuracy = 99.97%: 100%|██████████| 572/572 [00:06<00:00, 82.41it/s]\n",
            "[28 / 50]   Val: Loss = 0.06166, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 60.83it/s]\n",
            "[29 / 50] Train: Loss = 0.00009, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.04it/s]\n",
            "[29 / 50]   Val: Loss = 0.05968, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 56.70it/s]\n",
            "[30 / 50] Train: Loss = 0.00004, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.01it/s]\n",
            "[30 / 50]   Val: Loss = 0.06326, Accuracy = 96.21%: 100%|██████████| 13/13 [00:00<00:00, 58.14it/s]\n",
            "[31 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 81.91it/s]\n",
            "[31 / 50]   Val: Loss = 0.06761, Accuracy = 96.18%: 100%|██████████| 13/13 [00:00<00:00, 59.13it/s]\n",
            "[32 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 83.33it/s]\n",
            "[32 / 50]   Val: Loss = 0.06480, Accuracy = 96.19%: 100%|██████████| 13/13 [00:00<00:00, 61.26it/s]\n",
            "[33 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 81.19it/s]\n",
            "[33 / 50]   Val: Loss = 0.07243, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 60.68it/s]\n",
            "[34 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.62it/s]\n",
            "[34 / 50]   Val: Loss = 0.06600, Accuracy = 96.11%: 100%|██████████| 13/13 [00:00<00:00, 60.62it/s]\n",
            "[35 / 50] Train: Loss = 0.00107, Accuracy = 99.89%: 100%|██████████| 572/572 [00:06<00:00, 81.91it/s]\n",
            "[35 / 50]   Val: Loss = 0.06388, Accuracy = 96.04%: 100%|██████████| 13/13 [00:00<00:00, 55.36it/s]\n",
            "[36 / 50] Train: Loss = 0.00085, Accuracy = 99.91%: 100%|██████████| 572/572 [00:06<00:00, 82.27it/s]\n",
            "[36 / 50]   Val: Loss = 0.06767, Accuracy = 96.02%: 100%|██████████| 13/13 [00:00<00:00, 58.59it/s]\n",
            "[37 / 50] Train: Loss = 0.00018, Accuracy = 99.99%: 100%|██████████| 572/572 [00:06<00:00, 82.04it/s]\n",
            "[37 / 50]   Val: Loss = 0.06659, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 59.47it/s]\n",
            "[38 / 50] Train: Loss = 0.00005, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.53it/s]\n",
            "[38 / 50]   Val: Loss = 0.07136, Accuracy = 96.20%: 100%|██████████| 13/13 [00:00<00:00, 60.36it/s]\n",
            "[39 / 50] Train: Loss = 0.00003, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 81.97it/s]\n",
            "[39 / 50]   Val: Loss = 0.07055, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 60.48it/s]\n",
            "[40 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.49it/s]\n",
            "[40 / 50]   Val: Loss = 0.06912, Accuracy = 96.26%: 100%|██████████| 13/13 [00:00<00:00, 58.69it/s]\n",
            "[41 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 81.86it/s]\n",
            "[41 / 50]   Val: Loss = 0.07117, Accuracy = 96.25%: 100%|██████████| 13/13 [00:00<00:00, 57.49it/s]\n",
            "[42 / 50] Train: Loss = 0.00002, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 80.78it/s]\n",
            "[42 / 50]   Val: Loss = 0.07568, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 60.54it/s]\n",
            "[43 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 80.49it/s]\n",
            "[43 / 50]   Val: Loss = 0.07536, Accuracy = 96.27%: 100%|██████████| 13/13 [00:00<00:00, 59.36it/s]\n",
            "[44 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 81.55it/s]\n",
            "[44 / 50]   Val: Loss = 0.07442, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 60.14it/s]\n",
            "[45 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:06<00:00, 82.17it/s]\n",
            "[45 / 50]   Val: Loss = 0.07922, Accuracy = 96.28%: 100%|██████████| 13/13 [00:00<00:00, 62.40it/s]\n",
            "[46 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 81.34it/s]\n",
            "[46 / 50]   Val: Loss = 0.07839, Accuracy = 96.24%: 100%|██████████| 13/13 [00:00<00:00, 57.59it/s]\n",
            "[47 / 50] Train: Loss = 0.00001, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 81.33it/s]\n",
            "[47 / 50]   Val: Loss = 0.08273, Accuracy = 96.23%: 100%|██████████| 13/13 [00:00<00:00, 59.49it/s]\n",
            "[48 / 50] Train: Loss = 0.00162, Accuracy = 99.83%: 100%|██████████| 572/572 [00:06<00:00, 81.80it/s]\n",
            "[48 / 50]   Val: Loss = 0.06947, Accuracy = 96.17%: 100%|██████████| 13/13 [00:00<00:00, 58.81it/s]\n",
            "[49 / 50] Train: Loss = 0.00046, Accuracy = 99.96%: 100%|██████████| 572/572 [00:07<00:00, 79.92it/s]\n",
            "[49 / 50]   Val: Loss = 0.08257, Accuracy = 96.13%: 100%|██████████| 13/13 [00:00<00:00, 58.89it/s]\n",
            "[50 / 50] Train: Loss = 0.00008, Accuracy = 100.00%: 100%|██████████| 572/572 [00:07<00:00, 80.77it/s]\n",
            "[50 / 50]   Val: Loss = 0.07556, Accuracy = 96.22%: 100%|██████████| 13/13 [00:00<00:00, 57.67it/s]\n"
          ]
        }
      ],
      "source": [
        "class BidirectionalLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n",
        "        self.lstm = nn.LSTM(word_emb_dim, lstm_hidden_dim, num_layers=lstm_layers_count, bidirectional=True)\n",
        "        self.FC = nn.Linear(lstm_hidden_dim * 2, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        embedding = self.embedding(inputs)\n",
        "        out, _ = self.lstm(embedding)\n",
        "        return self.FC(out)\n",
        "\n",
        "model = BidirectionalLSTMTagger(\n",
        "    vocab_size=len(word2ind),\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkHJCV5zHkqg",
        "outputId": "bffe7d78-eca6-4e6f-e049-c676d66ab42f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9622989028089056"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTXmYGD_ANhm"
      },
      "source": [
        "### Предобученные эмбеддинги\n",
        "\n",
        "Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n",
        "\n",
        "Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZpY_Q1xZ18h",
        "outputId": "6d7a0f6c-354e-41bb-8afb-263e801a3bca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 128.1/128.1MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "w2v_model = api.load('glove-wiki-gigaword-100')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYogOoKlgtcf"
      },
      "source": [
        "Построим подматрицу для слов из нашей тренировочной выборки:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VsCstxiO03oT",
        "outputId": "cc5a7062-861b-4384-d02c-4cc2fc6a7b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Know 38736 out of 45441 word embeddings\n"
          ]
        }
      ],
      "source": [
        "known_count = 0\n",
        "embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n",
        "for word, ind in word2ind.items():\n",
        "    word = word.lower()\n",
        "    if word in w2v_model.vocab:\n",
        "        embeddings[ind] = w2v_model.get_vector(word)\n",
        "        known_count += 1\n",
        "        \n",
        "print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HcG7i-R8hbY3"
      },
      "source": [
        "**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "LxaRBpQd0pat"
      },
      "outputs": [],
      "source": [
        "class LSTMTaggerWithPretrainedEmbs(nn.Module):\n",
        "    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n",
        "        super().__init__()\n",
        "        embeddings = torch.FloatTensor(embeddings)\n",
        "        self.embedding =nn.Embedding.from_pretrained(embeddings)\n",
        "        self.lstm = nn.LSTM(embeddings.shape[1], lstm_hidden_dim, num_layers=lstm_layers_count)\n",
        "        self.FC = nn.Linear(lstm_hidden_dim, tagset_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        embedding = self.embedding(inputs)\n",
        "        out, _ = self.lstm(embedding)\n",
        "        return self.FC(out)        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBtI6BDE-Fc7",
        "outputId": "25deeebe-e727-4770-f911-ac13fe62f0a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[1 / 50] Train: Loss = 0.77277, Accuracy = 77.33%: 100%|██████████| 572/572 [00:04<00:00, 119.58it/s]\n",
            "[1 / 50]   Val: Loss = 0.37360, Accuracy = 89.02%: 100%|██████████| 13/13 [00:00<00:00, 83.62it/s]\n",
            "[2 / 50] Train: Loss = 0.28530, Accuracy = 91.42%: 100%|██████████| 572/572 [00:04<00:00, 121.29it/s]\n",
            "[2 / 50]   Val: Loss = 0.25462, Accuracy = 92.17%: 100%|██████████| 13/13 [00:00<00:00, 84.91it/s]\n",
            "[3 / 50] Train: Loss = 0.20806, Accuracy = 93.48%: 100%|██████████| 572/572 [00:04<00:00, 124.46it/s]\n",
            "[3 / 50]   Val: Loss = 0.20885, Accuracy = 93.46%: 100%|██████████| 13/13 [00:00<00:00, 87.72it/s]\n",
            "[4 / 50] Train: Loss = 0.17256, Accuracy = 94.47%: 100%|██████████| 572/572 [00:04<00:00, 121.91it/s]\n",
            "[4 / 50]   Val: Loss = 0.18456, Accuracy = 94.07%: 100%|██████████| 13/13 [00:00<00:00, 84.27it/s]\n",
            "[5 / 50] Train: Loss = 0.15207, Accuracy = 95.06%: 100%|██████████| 572/572 [00:04<00:00, 119.27it/s]\n",
            "[5 / 50]   Val: Loss = 0.16988, Accuracy = 94.47%: 100%|██████████| 13/13 [00:00<00:00, 81.11it/s]\n",
            "[6 / 50] Train: Loss = 0.13835, Accuracy = 95.45%: 100%|██████████| 572/572 [00:04<00:00, 120.40it/s]\n",
            "[6 / 50]   Val: Loss = 0.15980, Accuracy = 94.79%: 100%|██████████| 13/13 [00:00<00:00, 82.65it/s]\n",
            "[7 / 50] Train: Loss = 0.12875, Accuracy = 95.70%: 100%|██████████| 572/572 [00:04<00:00, 121.38it/s]\n",
            "[7 / 50]   Val: Loss = 0.15435, Accuracy = 94.93%: 100%|██████████| 13/13 [00:00<00:00, 84.48it/s]\n",
            "[8 / 50] Train: Loss = 0.12135, Accuracy = 95.92%: 100%|██████████| 572/572 [00:04<00:00, 118.29it/s]\n",
            "[8 / 50]   Val: Loss = 0.14895, Accuracy = 95.03%: 100%|██████████| 13/13 [00:00<00:00, 85.81it/s]\n",
            "[9 / 50] Train: Loss = 0.11579, Accuracy = 96.07%: 100%|██████████| 572/572 [00:04<00:00, 117.66it/s]\n",
            "[9 / 50]   Val: Loss = 0.14671, Accuracy = 95.13%: 100%|██████████| 13/13 [00:00<00:00, 84.36it/s]\n",
            "[10 / 50] Train: Loss = 0.11132, Accuracy = 96.22%: 100%|██████████| 572/572 [00:04<00:00, 118.52it/s]\n",
            "[10 / 50]   Val: Loss = 0.14284, Accuracy = 95.22%: 100%|██████████| 13/13 [00:00<00:00, 83.30it/s]\n",
            "[11 / 50] Train: Loss = 0.10741, Accuracy = 96.33%: 100%|██████████| 572/572 [00:04<00:00, 118.47it/s]\n",
            "[11 / 50]   Val: Loss = 0.14160, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 86.57it/s]\n",
            "[12 / 50] Train: Loss = 0.10404, Accuracy = 96.42%: 100%|██████████| 572/572 [00:04<00:00, 117.88it/s]\n",
            "[12 / 50]   Val: Loss = 0.13849, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 82.58it/s]\n",
            "[13 / 50] Train: Loss = 0.10134, Accuracy = 96.51%: 100%|██████████| 572/572 [00:04<00:00, 117.59it/s]\n",
            "[13 / 50]   Val: Loss = 0.13887, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 81.60it/s]\n",
            "[14 / 50] Train: Loss = 0.09867, Accuracy = 96.58%: 100%|██████████| 572/572 [00:04<00:00, 116.31it/s]\n",
            "[14 / 50]   Val: Loss = 0.13565, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 79.16it/s]\n",
            "[15 / 50] Train: Loss = 0.09645, Accuracy = 96.63%: 100%|██████████| 572/572 [00:04<00:00, 121.02it/s]\n",
            "[15 / 50]   Val: Loss = 0.13644, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 82.76it/s]\n",
            "[16 / 50] Train: Loss = 0.09443, Accuracy = 96.70%: 100%|██████████| 572/572 [00:04<00:00, 121.63it/s]\n",
            "[16 / 50]   Val: Loss = 0.13599, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 82.01it/s]\n",
            "[17 / 50] Train: Loss = 0.09242, Accuracy = 96.77%: 100%|██████████| 572/572 [00:04<00:00, 117.09it/s]\n",
            "[17 / 50]   Val: Loss = 0.13467, Accuracy = 95.49%: 100%|██████████| 13/13 [00:00<00:00, 82.91it/s]\n",
            "[18 / 50] Train: Loss = 0.09071, Accuracy = 96.82%: 100%|██████████| 572/572 [00:04<00:00, 118.74it/s]\n",
            "[18 / 50]   Val: Loss = 0.13491, Accuracy = 95.35%: 100%|██████████| 13/13 [00:00<00:00, 82.93it/s]\n",
            "[19 / 50] Train: Loss = 0.08915, Accuracy = 96.87%: 100%|██████████| 572/572 [00:04<00:00, 116.45it/s]\n",
            "[19 / 50]   Val: Loss = 0.13450, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 80.45it/s]\n",
            "[20 / 50] Train: Loss = 0.08755, Accuracy = 96.91%: 100%|██████████| 572/572 [00:04<00:00, 117.91it/s]\n",
            "[20 / 50]   Val: Loss = 0.13345, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 81.58it/s]\n",
            "[21 / 50] Train: Loss = 0.08622, Accuracy = 96.96%: 100%|██████████| 572/572 [00:05<00:00, 112.45it/s]\n",
            "[21 / 50]   Val: Loss = 0.13431, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 78.80it/s]\n",
            "[22 / 50] Train: Loss = 0.08481, Accuracy = 97.00%: 100%|██████████| 572/572 [00:04<00:00, 115.16it/s]\n",
            "[22 / 50]   Val: Loss = 0.13620, Accuracy = 95.46%: 100%|██████████| 13/13 [00:00<00:00, 81.68it/s]\n",
            "[23 / 50] Train: Loss = 0.08379, Accuracy = 97.03%: 100%|██████████| 572/572 [00:04<00:00, 116.60it/s]\n",
            "[23 / 50]   Val: Loss = 0.13417, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 79.79it/s]\n",
            "[24 / 50] Train: Loss = 0.08249, Accuracy = 97.08%: 100%|██████████| 572/572 [00:04<00:00, 116.33it/s]\n",
            "[24 / 50]   Val: Loss = 0.13596, Accuracy = 95.48%: 100%|██████████| 13/13 [00:00<00:00, 79.36it/s]\n",
            "[25 / 50] Train: Loss = 0.08153, Accuracy = 97.11%: 100%|██████████| 572/572 [00:04<00:00, 114.94it/s]\n",
            "[25 / 50]   Val: Loss = 0.13400, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 84.86it/s]\n",
            "[26 / 50] Train: Loss = 0.08039, Accuracy = 97.14%: 100%|██████████| 572/572 [00:05<00:00, 113.97it/s]\n",
            "[26 / 50]   Val: Loss = 0.13531, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 80.05it/s]\n",
            "[27 / 50] Train: Loss = 0.07931, Accuracy = 97.18%: 100%|██████████| 572/572 [00:04<00:00, 118.04it/s]\n",
            "[27 / 50]   Val: Loss = 0.13434, Accuracy = 95.41%: 100%|██████████| 13/13 [00:00<00:00, 76.92it/s]\n",
            "[28 / 50] Train: Loss = 0.07860, Accuracy = 97.21%: 100%|██████████| 572/572 [00:04<00:00, 117.12it/s]\n",
            "[28 / 50]   Val: Loss = 0.13547, Accuracy = 95.50%: 100%|██████████| 13/13 [00:00<00:00, 80.27it/s]\n",
            "[29 / 50] Train: Loss = 0.07756, Accuracy = 97.24%: 100%|██████████| 572/572 [00:04<00:00, 115.49it/s]\n",
            "[29 / 50]   Val: Loss = 0.13500, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 80.89it/s]\n",
            "[30 / 50] Train: Loss = 0.07663, Accuracy = 97.28%: 100%|██████████| 572/572 [00:05<00:00, 112.98it/s]\n",
            "[30 / 50]   Val: Loss = 0.13698, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 74.57it/s]\n",
            "[31 / 50] Train: Loss = 0.07591, Accuracy = 97.31%: 100%|██████████| 572/572 [00:05<00:00, 111.20it/s]\n",
            "[31 / 50]   Val: Loss = 0.13593, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 79.88it/s]\n",
            "[32 / 50] Train: Loss = 0.07512, Accuracy = 97.33%: 100%|██████████| 572/572 [00:04<00:00, 114.99it/s]\n",
            "[32 / 50]   Val: Loss = 0.13525, Accuracy = 95.43%: 100%|██████████| 13/13 [00:00<00:00, 77.94it/s]\n",
            "[33 / 50] Train: Loss = 0.07424, Accuracy = 97.36%: 100%|██████████| 572/572 [00:04<00:00, 114.44it/s]\n",
            "[33 / 50]   Val: Loss = 0.13610, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 81.49it/s]\n",
            "[34 / 50] Train: Loss = 0.07369, Accuracy = 97.39%: 100%|██████████| 572/572 [00:05<00:00, 111.97it/s]\n",
            "[34 / 50]   Val: Loss = 0.13775, Accuracy = 95.37%: 100%|██████████| 13/13 [00:00<00:00, 74.83it/s]\n",
            "[35 / 50] Train: Loss = 0.07283, Accuracy = 97.41%: 100%|██████████| 572/572 [00:05<00:00, 113.03it/s]\n",
            "[35 / 50]   Val: Loss = 0.13682, Accuracy = 95.39%: 100%|██████████| 13/13 [00:00<00:00, 76.40it/s]\n",
            "[36 / 50] Train: Loss = 0.07185, Accuracy = 97.45%: 100%|██████████| 572/572 [00:05<00:00, 113.98it/s]\n",
            "[36 / 50]   Val: Loss = 0.13892, Accuracy = 95.33%: 100%|██████████| 13/13 [00:00<00:00, 80.78it/s]\n",
            "[37 / 50] Train: Loss = 0.07136, Accuracy = 97.46%: 100%|██████████| 572/572 [00:05<00:00, 113.99it/s]\n",
            "[37 / 50]   Val: Loss = 0.14021, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 78.53it/s]\n",
            "[38 / 50] Train: Loss = 0.07054, Accuracy = 97.50%: 100%|██████████| 572/572 [00:04<00:00, 116.51it/s]\n",
            "[38 / 50]   Val: Loss = 0.13699, Accuracy = 95.34%: 100%|██████████| 13/13 [00:00<00:00, 78.70it/s]\n",
            "[39 / 50] Train: Loss = 0.06996, Accuracy = 97.50%: 100%|██████████| 572/572 [00:04<00:00, 117.84it/s]\n",
            "[39 / 50]   Val: Loss = 0.14115, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 79.71it/s]\n",
            "[40 / 50] Train: Loss = 0.06939, Accuracy = 97.53%: 100%|██████████| 572/572 [00:05<00:00, 114.35it/s]\n",
            "[40 / 50]   Val: Loss = 0.14083, Accuracy = 95.24%: 100%|██████████| 13/13 [00:00<00:00, 75.54it/s]\n",
            "[41 / 50] Train: Loss = 0.06868, Accuracy = 97.55%: 100%|██████████| 572/572 [00:05<00:00, 113.09it/s]\n",
            "[41 / 50]   Val: Loss = 0.14137, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 77.63it/s]\n",
            "[42 / 50] Train: Loss = 0.06824, Accuracy = 97.58%: 100%|██████████| 572/572 [00:04<00:00, 117.11it/s]\n",
            "[42 / 50]   Val: Loss = 0.14074, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 77.97it/s]\n",
            "[43 / 50] Train: Loss = 0.06746, Accuracy = 97.61%: 100%|██████████| 572/572 [00:05<00:00, 110.45it/s]\n",
            "[43 / 50]   Val: Loss = 0.14175, Accuracy = 95.36%: 100%|██████████| 13/13 [00:00<00:00, 76.56it/s]\n",
            "[44 / 50] Train: Loss = 0.06707, Accuracy = 97.62%: 100%|██████████| 572/572 [00:05<00:00, 111.40it/s]\n",
            "[44 / 50]   Val: Loss = 0.14207, Accuracy = 95.26%: 100%|██████████| 13/13 [00:00<00:00, 76.35it/s]\n",
            "[45 / 50] Train: Loss = 0.06632, Accuracy = 97.65%: 100%|██████████| 572/572 [00:04<00:00, 114.76it/s]\n",
            "[45 / 50]   Val: Loss = 0.14375, Accuracy = 95.23%: 100%|██████████| 13/13 [00:00<00:00, 80.49it/s]\n",
            "[46 / 50] Train: Loss = 0.06589, Accuracy = 97.65%: 100%|██████████| 572/572 [00:05<00:00, 113.53it/s]\n",
            "[46 / 50]   Val: Loss = 0.14388, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 77.95it/s]\n",
            "[47 / 50] Train: Loss = 0.06534, Accuracy = 97.68%: 100%|██████████| 572/572 [00:05<00:00, 112.93it/s]\n",
            "[47 / 50]   Val: Loss = 0.14472, Accuracy = 95.27%: 100%|██████████| 13/13 [00:00<00:00, 76.67it/s]\n",
            "[48 / 50] Train: Loss = 0.06479, Accuracy = 97.70%: 100%|██████████| 572/572 [00:05<00:00, 111.84it/s]\n",
            "[48 / 50]   Val: Loss = 0.14725, Accuracy = 95.31%: 100%|██████████| 13/13 [00:00<00:00, 76.42it/s]\n",
            "[49 / 50] Train: Loss = 0.06422, Accuracy = 97.71%: 100%|██████████| 572/572 [00:05<00:00, 110.95it/s]\n",
            "[49 / 50]   Val: Loss = 0.14621, Accuracy = 95.32%: 100%|██████████| 13/13 [00:00<00:00, 79.97it/s]\n",
            "[50 / 50] Train: Loss = 0.06383, Accuracy = 97.74%: 100%|██████████| 572/572 [00:04<00:00, 114.79it/s]\n",
            "[50 / 50]   Val: Loss = 0.14760, Accuracy = 95.38%: 100%|██████████| 13/13 [00:00<00:00, 77.89it/s]\n"
          ]
        }
      ],
      "source": [
        "model = LSTMTaggerWithPretrainedEmbs(\n",
        "    embeddings=embeddings,\n",
        "    tagset_size=len(tag2ind)\n",
        ").cuda()\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n",
        "    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Ne_8f24h8kg"
      },
      "source": [
        "**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n",
        "\n",
        "Добейтесь качества лучше прошлых моделей."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUuAPGhEGVR",
        "outputId": "338a64e6-c549-445d-e470-a3392cf46cda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9536955750871866"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "correct_count = 0\n",
        "sum_count = 0\n",
        "\n",
        "for X_batch, y_batch in iterate_batches((X_test, y_test), 64):\n",
        "    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n",
        "    logits = model(X_batch)\n",
        "\n",
        "    preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "    mask = (y_batch != 0).float()\n",
        "    cur_correct_count = ((preds == y_batch).float() * mask).sum().item()\n",
        "    cur_sum_count = mask.sum().item()\n",
        "                \n",
        "    correct_count += cur_correct_count\n",
        "    sum_count += cur_sum_count\n",
        "\n",
        "correct_count / sum_count"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "RNNs.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}